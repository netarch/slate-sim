{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import threading\n",
    "import sys\n",
    "import time\n",
    "from utils import utils\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "unique_timestamp = list()\n",
    "for ts in range(0, 43260000, 60000):\n",
    "    unique_timestamp.append(str(int(ts/60000)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_msname_metric_df(df_, msname_list_):\n",
    "    msname_provider_mcr_ = dict()\n",
    "    for msname_ in msname_list_:\n",
    "        msname_provider_mcr_[msname_] = list()\n",
    "        msname_df_ = df_[df_[\"msname\"] == msname_]\n",
    "        for ts in unique_timestamp:\n",
    "            temp_df_ = msname_df_[msname_df_['timestamp'] == ts]\n",
    "            temp_df_ = temp_df_[temp_df_[\"metric\"] == \"providerRPC_MCR\"]\n",
    "            msname_provider_mcr_[msname_].append(temp_df_)\n",
    "    # msname_provider_mcr_ => { msname : [mcr at t=0, mcr at t=1, ...]}\n",
    "    return msname_provider_mcr_\n",
    "\n",
    "\n",
    "def get_statistics(li_):\n",
    "    temp_li = list()\n",
    "    temp_li.append(len(li_))\n",
    "    temp_li.append(sum(li_))\n",
    "    temp_li.append(np.average(li_))\n",
    "    temp_li.append(np.std(li_))\n",
    "    temp_li.append(min(li_))\n",
    "    temp_li.append(max(li_))\n",
    "    if min(li_) == 0:\n",
    "        temp_li.append(float('inf'))\n",
    "    else:\n",
    "        temp_li.append(max(li_)/min(li_))\n",
    "    if np.percentile(li_, 1) == 0:\n",
    "        temp_li.append(float('inf'))\n",
    "    else:\n",
    "        temp_li.append(max(li_)/np.percentile(li_, 1))\n",
    "    temp_li.append(np.percentile(li_, 0.1))\n",
    "    temp_li.append(np.percentile(li_, 1))\n",
    "    temp_li.append(np.percentile(li_, 5))\n",
    "    temp_li.append(np.percentile(li_, 10))\n",
    "    temp_li.append(np.percentile(li_, 25))\n",
    "    temp_li.append(np.percentile(li_, 50))\n",
    "    temp_li.append(np.percentile(li_, 75))\n",
    "    temp_li.append(np.percentile(li_, 90))\n",
    "    temp_li.append(np.percentile(li_, 95))\n",
    "    temp_li.append(np.percentile(li_, 99))\n",
    "    temp_li.append(np.percentile(li_, 99.9))\n",
    "    return temp_li\n",
    "\n",
    "def generate_stat_df(msname_provider_mcr_, msname_, output_df_):\n",
    "    for i in range(len(unique_timestamp)):\n",
    "        temp_df = msname_provider_mcr_[msname_][i]\n",
    "        v_ = temp_df[\"value\"].values.tolist()\n",
    "        if sum(v_) == -1:\n",
    "            continue\n",
    "        if  -1 in v_:\n",
    "            v_.remove(-1)\n",
    "        temp_row = [msname_, str(unique_timestamp[i])]\n",
    "        if len(v_) != 0:\n",
    "            stats = get_statistics(v_)\n",
    "            temp_row.extend(stats)\n",
    "            output_df_.loc[len(output_df_.index)] = temp_row\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "def detect_microburst(call_rate, window_size, burst_degree):\n",
    "    burst_ts = list()\n",
    "    burst_cnt = 0\n",
    "    burst_detect = 0\n",
    "    for i in range(window_size, len(call_rate)):\n",
    "        if burst_detect == 0:\n",
    "            prev_window_avg = sum(call_rate[i-window_size:i])/window_size\n",
    "            if call_rate[i] > prev_window_avg * burst_degree:\n",
    "                burst_cnt += 1\n",
    "                burst_ts.append(i)\n",
    "                burst_detect = window_size\n",
    "        else:\n",
    "            burst_detect -= 1\n",
    "    return burst_cnt, burst_ts\n",
    "\n",
    "def series_string_to_list(series):\n",
    "    # series = series.tolist()[0]\n",
    "    to_li = series[1:-1]\n",
    "    to_li = to_li.split(\",\")\n",
    "    to_li = [float(x) for x in to_li]\n",
    "    return to_li\n",
    "\n",
    "\n",
    "def read_call_rate(file_path):\n",
    "    file1 = open(file_path, 'r')\n",
    "    lines = file1.readlines()\n",
    "    file1.close()\n",
    "    msname = lines.pop(0)\n",
    "    call_rate = list()\n",
    "    for i in range(len(lines)):\n",
    "        call_rate.append(float(lines[i]))\n",
    "    return msname, call_rate\n",
    "    \n",
    "########################################################################################## \n",
    "    \n",
    "class WorkloadGenerator:\n",
    "    def __init__ (self, req_per_sec, total_sec):\n",
    "        self.request_per_sec = req_per_sec\n",
    "        self.total_seconds = total_sec\n",
    "        self.total_num_req = self.request_per_sec * self.total_seconds\n",
    "        self.total_num_req = int(self.total_num_req)\n",
    "\n",
    "    def exponential_distribution(self):\n",
    "        scale_ = (1 / self.request_per_sec) # scale parameter is the inverse of the rate parameter lambda\n",
    "        exp_dist=np.random.exponential(scale=scale_, size=(self.total_num_req))\n",
    "        exp_dist_sum = sum(exp_dist)\n",
    "        total_millisecond = self.total_seconds*1000\n",
    "        weight = total_millisecond / exp_dist_sum\n",
    "        norm_exp_dist = [ x*weight for x in exp_dist ] # norm + sec->ms\n",
    "        # norm_exp_dist = [ (x / sum(exp_dist))*self.total_seconds*1000 for x in exp_dist ] # norm + sec->ms\n",
    "        # first_request_start_time = 0.0\n",
    "        # norm_exp_dist.insert(0, first_request_start_time) # For the very first event, 0 second event arrival is inserted.\n",
    "        \n",
    "        # print(\"DEBUG\", \"\")\n",
    "        # print(\"DEBUG\", \"=\"*40)\n",
    "        # print(\"DEBUG\", \"== Exponential workload statistics ==\")\n",
    "        # print(\"DEBUG\", \"=\"*40)\n",
    "        # print(\"DEBUG\", \"- total num requests: {}\".format(self.total_num_req))\n",
    "        # print(\"DEBUG\", \"- sum: {}\".format(sum(norm_exp_dist)))\n",
    "        # print(\"DEBUG\", \"- mean interval: {}\".format(sum(norm_exp_dist)/len(norm_exp_dist)))\n",
    "        # print(\"DEBUG\", \"- max interval: {}\".format(max(norm_exp_dist)))\n",
    "        # print(\"DEBUG\", \"- min interval: {}\".format(min(norm_exp_dist)))\n",
    "        # print(\"DEBUG\", \"=\"*40)\n",
    "        return norm_exp_dist\n",
    "    \n",
    "    def constant_distribution(self):\n",
    "        dist = [(self.total_seconds/self.total_num_req) * 1000] * self.total_num_req\n",
    "        # print(\"DEBUG\", \"total_num_req: \", self.total_num_req)\n",
    "        # print(\"DEBUG\", dist)\n",
    "        # print(\"DEBUG\", \"\")\n",
    "        # print(\"DEBUG\", \"=\"*40)\n",
    "        # print(\"DEBUG\", \"== Constant workload statistics ==\")\n",
    "        # print(\"DEBUG\", \"=\"*40)\n",
    "        # print(\"DEBUG\", \"- total num requests: {}\".format(self.total_num_req))\n",
    "        # print(\"DEBUG\", \"- sum: {}\".format(sum(dist)))\n",
    "        # print(\"DEBUG\", \"- mean interval: {}\".format(sum(dist)/len(dist)))\n",
    "        # print(\"DEBUG\", \"- max interval: {}\".format(max(dist)))\n",
    "        # print(\"DEBUG\", \"- min interval: {}\".format(min(dist)))\n",
    "        # print(\"DEBUG\", \"=\"*40)\n",
    "        return dist\n",
    "    \n",
    "def interval_to_arrival(req_intv):\n",
    "    req_arrival = list()\n",
    "    for i in range(len(req_intv)):\n",
    "        if i == 0:\n",
    "            req_arrival.append(req_intv[i])\n",
    "        else:\n",
    "            req_arrival.append(req_arrival[i-1] + req_intv[i])\n",
    "    return req_arrival\n",
    "\n",
    "def generate_workload_from_alibaba_trace(rpm, base_rps):\n",
    "    def normalize(rps_list, base_rps):\n",
    "        min_rps = np.percentile(rps_list, 50)\n",
    "        norm_weight = base_rps/min_rps\n",
    "        return [x*norm_weight for x in rps_list]\n",
    "    rps_list = [ x/60 for x in rpm]\n",
    "    norm_rps_list = normalize(rps_list, base_rps)\n",
    "    wrk_list = list()\n",
    "    for rps in norm_rps_list:\n",
    "        wrk_list.append(WorkloadGenerator(req_per_sec=rps, total_sec=60))\n",
    "    request_interval = list()\n",
    "    for i in range(len(wrk_list)):\n",
    "        request_interval += wrk_list[i].exponential_distribution()\n",
    "    return request_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_all_svc:  1043\n",
      "num_bursty_svc:  285\n",
      "total_num_burst:  3079\n",
      "average num of burst per all service during 12 hours: 2.9520613614573348\n",
      "average num of burst per bursty service during 12 hours: 10.803508771929824\n",
      "average num of burst per all service per hour: 0.2460051134547779\n",
      "average num of burst per bursty service per hour: 0.9002923976608187\n"
     ]
    }
   ],
   "source": [
    "svc_per_line_df = pd.read_csv(\"svc_per_line.csv\")\n",
    "\n",
    "''' Count burstness '''\n",
    "num_burst = list()\n",
    "burst_timestamp = list()\n",
    "window_size=5\n",
    "burst_degree=2\n",
    "for idx in range(len(svc_per_line_df)):\n",
    "    rpm = svc_per_line_df.loc[idx, \"li\"]\n",
    "    rpm = series_string_to_list(rpm)\n",
    "    burst_cnt, burst_ts = detect_microburst(rpm, window_size, burst_degree)\n",
    "    num_burst.append(burst_cnt)\n",
    "    burst_timestamp.append(burst_ts)\n",
    "svc_per_line_df[\"num_burst\"] = num_burst\n",
    "svc_per_line_df[\"burst_timestamp\"] = burst_timestamp\n",
    "svc_per_line_df = svc_per_line_df.sort_values(by=[\"num_burst\"], ascending=False)\n",
    "svc_per_line_df = svc_per_line_df.reset_index(drop=True)\n",
    "\n",
    "''' Burst statistics '''\n",
    "total_num_burst = svc_per_line_df['num_burst'].sum() # 2703\n",
    "bursty_svc_df = svc_per_line_df[svc_per_line_df[\"num_burst\"] > 0]\n",
    "num_bursty_svc = len(bursty_svc_df) # 264 out of 1301\n",
    "num_all_svc = len(svc_per_line_df) # 264 out of 1301\n",
    "avg_burst_per_svc = total_num_burst/num_all_svc\n",
    "avg_burst_per_bursty_svc = total_num_burst/num_bursty_svc\n",
    "print(\"num_all_svc: \", num_all_svc)\n",
    "print(\"num_bursty_svc: \", num_bursty_svc)\n",
    "print(\"total_num_burst: \", total_num_burst)\n",
    "print(\"average num of burst per all service during 12 hours: {}\".format(avg_burst_per_svc))\n",
    "print(\"average num of burst per bursty service during 12 hours: {}\".format(avg_burst_per_bursty_svc))\n",
    "print(\"average num of burst per all service per hour: {}\".format(avg_burst_per_svc/12))\n",
    "print(\"average num of burst per bursty service per hour: {}\".format(avg_burst_per_bursty_svc/12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request arrival file write: 3.6806602478027344\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "msname_8 = \"6d9c26b9\"\n",
    "\n",
    "\n",
    "rpm = svc_per_line_df[svc_per_line_df[\"msname-8\"] == msname_8][\"li\"].tolist()[0]\n",
    "rpm = series_string_to_list(rpm)\n",
    "request_interval = generate_workload_from_alibaba_trace(rpm, 50)\n",
    "request_arrival = interval_to_arrival(request_interval)\n",
    "ts = time.time()\n",
    "temp = list()\n",
    "for arr_time in request_arrival:\n",
    "    temp.append(str(arr_time)+\"\\n\")\n",
    "file1 = open(\"request_arrival_time_\"+msname_8+\".txt\", 'w')\n",
    "file1.writelines(temp)\n",
    "file1.close()\n",
    "print(\"request arrival file write: {}\".format(time.time() - ts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_base_rps:  8\n",
      "p50_rps:  12.0\n",
      "norm_weight:  1.5\n",
      "p50_rps_of_bursty_cluster:  16.0\n",
      "target_rps:  8\n",
      "total_seconds:  43201\n",
      "wrk\n",
      "exp function overhead:  0.15987348556518555\n"
     ]
    }
   ],
   "source": [
    "def extract_rps_from_request_arrival(req_arr):\n",
    "    rps_list = list()\n",
    "    rps = 0\n",
    "    window = 0\n",
    "    for i in range(1, len(req_arr)):\n",
    "        rps += 1\n",
    "        if req_arr[i] >= 1000*window:\n",
    "            rps_list.append(rps)                    \n",
    "            rps = 0\n",
    "            window += 1\n",
    "    return rps_list\n",
    "\n",
    "def thin_the_request_arrival_list(req_arr, target_rps):\n",
    "    rps_list = extract_rps_from_request_arrival(req_arr)\n",
    "    p50_rps = np.percentile(rps_list, 2) # NOTE: Hardcoded. Apparently, 2 percentile is not p50.\n",
    "    norm_weight = p50_rps/target_rps\n",
    "    print(\"target_base_rps: \", target_rps)\n",
    "    print(\"p50_rps: \", p50_rps)\n",
    "    print(\"norm_weight: \", norm_weight)\n",
    "    thin_req_arr = list()\n",
    "    for i in range(len(req_arr)):\n",
    "        if i%norm_weight==0:\n",
    "            thin_req_arr.append(req_arr[i])\n",
    "    return thin_req_arr\n",
    "\n",
    "def generate_non_bursty_cluster_workload(duration_in_sec, target_rps):\n",
    "    wrk = WorkloadGenerator(req_per_sec=target_rps, total_sec=duration_in_sec)\n",
    "    print(\"wrk\")\n",
    "    ts = time.time()\n",
    "    non_bursty_req_interval = wrk.exponential_distribution()\n",
    "    print(\"exp function overhead: \", time.time() - ts)\n",
    "    req_arr = interval_to_arrival(non_bursty_req_interval)\n",
    "    return req_arr\n",
    "\n",
    "def calc_initial_one_service_num_replica(rps_list, factor):\n",
    "    first_five_minute_rps = rps_list[:1]\n",
    "    avg_rps = sum(first_five_minute_rps)/len(first_five_minute_rps)\n",
    "    per_replica_capacity = 10 # 100ms processing time\n",
    "    initial_num_replica = int((avg_rps/per_replica_capacity) * factor)\n",
    "    return initial_num_replica\n",
    "\n",
    "file1 = open(\"request_arrival_time_\"+msname_8+\".txt\", 'r')\n",
    "lines = file1.readlines()\n",
    "file1.close()\n",
    "req_arrival_time_0 = list()\n",
    "for i in range(len(lines)):\n",
    "    req_arrival_time_0.append(float(lines[i]))\n",
    "req_arrival_time_0 = copy.deepcopy(req_arrival_time_0)\n",
    "target_rps = 8 # NOTE: Hardcoded\n",
    "req_arrival_time_0 = thin_the_request_arrival_list(req_arrival_time_0, target_rps)\n",
    "bursty_cluster_rps_list = extract_rps_from_request_arrival(req_arrival_time_0)\n",
    "total_seconds = len(bursty_cluster_rps_list)\n",
    "print(\"p50_rps_of_bursty_cluster: \", np.percentile(bursty_cluster_rps_list, 50))\n",
    "print(\"target_rps: \", target_rps)\n",
    "print(\"total_seconds: \", total_seconds)\n",
    "req_arrival_time_1 = generate_non_bursty_cluster_workload(total_seconds, np.percentile(bursty_cluster_rps_list, 50))\n",
    "\n",
    "\n",
    "def file_write_float_list(fname, li):\n",
    "    f_ = open(fname, 'w')\n",
    "    li = [str(x)+\"\\n\" for x in li]\n",
    "    f_.writelines(li)\n",
    "    f_.close()\n",
    "    \n",
    "fn = \"final_request_arrival_time_clsuter_0-\"+msname_8+\".txt\"\n",
    "file_write_float_list(fn, req_arrival_time_0)\n",
    "fn = \"final_request_arrival_time_clsuter_1-\"+msname_8+\".txt\"\n",
    "file_write_float_list(fn, req_arrival_time_1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
